{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DoAlexis/DoAlexis/blob/main/labs/ex02/template/Lab%202%20-%20Gradient%20Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W-HyC2ZqHv3v"
      },
      "outputs": [],
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A8mVglViHv3z",
        "outputId": "44874cc0-a5c9-4636-df1d-0049afb2142e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are running Python 3. Good job :)\n"
          ]
        }
      ],
      "source": [
        "# Check the Python version\n",
        "import sys\n",
        "if sys.version.startswith(\"3.\"):\n",
        "  print(\"You are running Python 3. Good job :)\")\n",
        "else:\n",
        "  print(\"This notebook requires Python 3.\\nIf you are using Google Colab, go to Runtime > Change runtime type and choose Python 3.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "40WN44vKHv30",
        "outputId": "4b4bc986-034a-491b-cb50-d98a43c2be33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cloned-repo'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 2663, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 2663 (delta 135), reused 144 (delta 108), pack-reused 2474 (from 3)\u001b[K\n",
            "Receiving objects: 100% (2663/2663), 483.96 MiB | 33.36 MiB/s, done.\n",
            "Resolving deltas: 100% (1315/1315), done.\n",
            "/content/cloned-repo/labs/ex02/template\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "  # Clone the entire repo to access the files.\n",
        "  !git clone -l -s https://github.com/epfml/OptML_course.git cloned-repo\n",
        "  %cd cloned-repo/labs/ex02/template/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwY_DoMpHv30"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2vYdD08Hv31"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from helpers import *\n",
        "\n",
        "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
        "x, mean_x, std_x = standardize(height)\n",
        "b, A = build_model_data(x, weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mar-mmHQHv32"
      },
      "outputs": [],
      "source": [
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuxSmf2IHv33"
      },
      "source": [
        "# Least Squares Estimation\n",
        "Least squares estimation is one of the fundamental machine learning algorithms. Given an $ n \\times d $ matrix $A$ and a $ n \\times 1$ vector $b$, the goal is to find a vector $x \\in \\mathbb{R}^d$ which minimizes the objective function $$f(x) = \\frac{1}{2n} \\sum_{i=1}^{n} (a_i^\\top x - b_i)^2 = \\frac{1}{2n} \\|Ax - b\\|^2 $$\n",
        "\n",
        "In this exercise, we will try to fit $x$ using Least Squares Estimation.\n",
        "\n",
        "One can see the function is $L$ smooth with $L =\\frac1n\\|A^T A\\|  = \\frac1n\\|A\\|^2$ (Lemma 2.3 for the first equality, and a few manipulations for the second)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcgaXCA9Hv33"
      },
      "source": [
        "# Computing the Objective Function\n",
        "Fill in the `calculate_objective` function below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mteKMjjJHv34"
      },
      "outputs": [],
      "source": [
        "def calculate_objective(Axmb):\n",
        "    \"\"\"Calculate the mean squared error for vector Axmb = Ax - b.\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute mean squared error\n",
        "    # ***************************************************\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNbV3c57Hv34"
      },
      "source": [
        "# Compute smoothness constant $L$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVuW0m7LHv34"
      },
      "source": [
        "To compute the spectral norm of A you can use np.linalg.norm(A, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePqnoPQLHv34"
      },
      "outputs": [],
      "source": [
        "def calculate_L(b, A):\n",
        "    \"\"\"Calculate the smoothness constant for f\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute ||A.T*A||\n",
        "    # ***************************************************\n",
        "    raise NotImplementedError\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute L = smoothness constant of f\n",
        "    # ***************************************************\n",
        "    raise NotImplementedError\n",
        "    return L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuxIFi0QHv35"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqaTJMjPHv35"
      },
      "source": [
        "Please fill in the functions `compute_gradient` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRFzOrp2Hv35"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(b, A, x):\n",
        "    \"\"\"Compute the gradient.\"\"\"\n",
        "    # ***************************************************\n",
        "    # INSERT YOUR CODE HERE\n",
        "    # TODO: compute gradient and objective\n",
        "    # ***************************************************\n",
        "    raise NotImplementedError\n",
        "    return grad, Axmb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN-WlR18Hv35"
      },
      "source": [
        "Please fill in the functions `gradient_descent` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq_0bwyFHv35"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(b, A, initial_x, max_iters, gamma):\n",
        "    \"\"\"Gradient descent algorithm.\"\"\"\n",
        "    # Define parameters to store x and objective func. values\n",
        "    xs = [initial_x]\n",
        "    objectives = []\n",
        "    x = initial_x\n",
        "    for n_iter in range(max_iters):\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: compute gradient and objective function\n",
        "        # ***************************************************\n",
        "        raise NotImplementedError\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: update x by a gradient descent step\n",
        "        # ***************************************************\n",
        "        raise NotImplementedError\n",
        "        # store x and objective function value\n",
        "        xs.append(x)\n",
        "        objectives.append(obj)\n",
        "        print(\"Gradient Descent({bi}/{ti}): objective={l}\".format(\n",
        "              bi=n_iter, ti=max_iters - 1, l=obj))\n",
        "\n",
        "    return objectives, xs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UMQR-ATHv36"
      },
      "source": [
        "Test your gradient descent function with a naive step size through gradient descent demo shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zUVussAHv36"
      },
      "outputs": [],
      "source": [
        "# from gradient_descent import *\n",
        "from plots import gradient_descent_visualization\n",
        "\n",
        "# Define the parameters of the algorithm.\n",
        "max_iters = 50\n",
        "\n",
        "gamma = 0.1\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives_naive, gradient_xs_naive = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO2DqeN-Hv36"
      },
      "source": [
        "Time Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzgP52nvHv36"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import IntSlider, interact\n",
        "from grid_search import *\n",
        "\n",
        "def plot_figure(n_iter):\n",
        "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
        "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
        "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
        "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
        "\n",
        "    fig = gradient_descent_visualization(\n",
        "        gradient_objectives_naive, gradient_xs_naive, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
        "    fig.set_size_inches(10.0, 6.0)\n",
        "\n",
        "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs_naive)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEh9dr2OHv36"
      },
      "source": [
        "Try doing gradient descent with a better learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "313cg8sMHv37"
      },
      "outputs": [],
      "source": [
        "# Define the parameters of the algorithm.\n",
        "max_iters = 50\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: a better learning rate using the smoothness of f\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfr4Tt5YHv37"
      },
      "source": [
        "Time visualization with a better learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mujR3ZLHv37"
      },
      "outputs": [],
      "source": [
        "def plot_figure(n_iter):\n",
        "    # Generate grid data for visualization (parameters to be swept and best combination)\n",
        "    grid_x0, grid_x1 = generate_w(num_intervals=10)\n",
        "    grid_objectives = grid_search(b, A, grid_x0, grid_x1)\n",
        "    obj_star, x0_star, x1_star = get_best_parameters(grid_x0, grid_x1, grid_objectives)\n",
        "\n",
        "    fig = gradient_descent_visualization(\n",
        "        gradient_objectives, gradient_xs, grid_objectives, grid_x0, grid_x1, mean_x, std_x, height, weight, n_iter)\n",
        "    fig.set_size_inches(10.0, 6.0)\n",
        "\n",
        "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_xs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MsZm3noHv37"
      },
      "source": [
        "# Loading more complex data\n",
        "The data is taken from https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUuuRtQLHv37"
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"Concrete_Data.csv\",delimiter=\",\")\n",
        "\"\"\"Note that running this part will change the A above, and it will cause error in the interaction plot!\"\"\"\n",
        "A = data[:,:-1]\n",
        "b = data[:,-1]\n",
        "A, mean_A, std_A = standardize(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l3CnrrQHv37"
      },
      "outputs": [],
      "source": [
        "print('Number of samples n = ', b.shape[0])\n",
        "print('Dimension of each sample d = ', A.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql_EiqI2Hv37"
      },
      "source": [
        "# Running gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNh-NniTHv38"
      },
      "source": [
        "## Assuming bounded gradients\n",
        "Assume we are moving in a bounded region $\\|x\\| \\leq 25$ containing all iterates (and we assume $\\|x-x^\\star\\| \\leq 25$ as well, for simplicity). Then by $\\nabla f(x) = \\frac{1}{n}A^\\top (Ax - b)$, one can see that $f$ is Lipschitz over that bounded region, with Lipschitz constant $\\|\\nabla f(x)\\| \\leq \\frac{1}{n} (\\|A^\\top A\\|\\|x\\| + \\|A^\\top b\\|)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lks4Hd44Hv38"
      },
      "outputs": [],
      "source": [
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: Compute the bound on the gradient norm\n",
        "# ***************************************************\n",
        "grad_norm_bound =\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-4bH8DmHv38"
      },
      "source": [
        "Fill in the learning rate assuming bounded gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vxst0unHv38"
      },
      "outputs": [],
      "source": [
        "max_iters = 50\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: Compute learning rate based on bounded gradient\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "bd_gradient_objectives, bd_gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))\n",
        "\n",
        "# Averaging the iterates as is the case for bounded gradients case\n",
        "bd_gradient_objectives_averaged = []\n",
        "for i in range(len(bd_gradient_xs)):\n",
        "    if i > 0:\n",
        "        bd_gradient_xs[i] = (i * bd_gradient_xs[i-1] + bd_gradient_xs[i])/(i + 1)\n",
        "    grad, err = compute_gradient(b, A, bd_gradient_xs[i])\n",
        "    obj = calculate_objective(err)\n",
        "    bd_gradient_objectives_averaged.append(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv1iUOFWHv38"
      },
      "source": [
        "## Gradient descent using smoothness\n",
        "Fill in the learning rate using smoothness of the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlSzQeXCHv38"
      },
      "outputs": [],
      "source": [
        "max_iters = 50\n",
        "\n",
        "\n",
        "# ***************************************************\n",
        "# INSERT YOUR CODE HERE\n",
        "# TODO: a better learning rate using the smoothness of f\n",
        "# ***************************************************\n",
        "gamma =\n",
        "raise NotImplementedError\n",
        "\n",
        "# Initialization\n",
        "x_initial = np.zeros(A.shape[1])\n",
        "\n",
        "# Start gradient descent.\n",
        "start_time = datetime.datetime.now()\n",
        "gradient_objectives, gradient_xs = gradient_descent(b, A, x_initial, max_iters, gamma)\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# Print result\n",
        "exection_time = (end_time - start_time).total_seconds()\n",
        "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obqFCh9QHv39"
      },
      "source": [
        "## Plotting the Evolution of the Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoBPm_tlHv39"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.xlabel('Number of steps')\n",
        "plt.ylabel('Objective Function')\n",
        "#plt.yscale(\"log\")\n",
        "plt.plot(range(len(gradient_objectives)), gradient_objectives,'r', label='gradient descent with 1/L stepsize')\n",
        "plt.plot(range(len(bd_gradient_objectives)), bd_gradient_objectives,'b', label='gradient descent assuming bounded gradients')\n",
        "plt.plot(range(len(bd_gradient_objectives_averaged)), bd_gradient_objectives_averaged,'g', label='gradient descent assuming bounded gradients with averaged iterates')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}